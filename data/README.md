# LLM Prompt Valence Study

This study investigates how varying prompt valence (neutral, supportive, and threatening) influences the quality of essay-style outputs from large language models (Claude Sonnet-3.5, OpenAI GPT4o, Gemini).

## Project Structure

### ğŸ“ `shared_data/` - Common Data and Results
- Prompt datasets
- Generated responses and evaluations
- Analysis results and logs

## Study Overview

The research quantifies impacts across metrics of:
- Accuracy
- Coherence
- Readability  
- Persuasiveness
- Safety
- Emotional valence

Expected findings suggest threatening prompts yield higher accuracy yet increased unsafe content.
