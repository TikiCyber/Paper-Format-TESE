{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.8.3)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "# If needed, run this once (comment out if your env already has these)\n",
    "# Note: may require restart after install in some notebook environments.\n",
    "!pip install --upgrade \"google-generativeai>=0.3.0\" pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Gemini client ready.\n"
     ]
    }
   ],
   "source": [
    "# Robust Google Gemini client init for Jupyter\n",
    "# - Sanitizes pasted key (removes spaces/newlines/zero-width chars)\n",
    "# - Does not hard-fail on prefix; only warns\n",
    "\n",
    "import os, sys, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, Dict\n",
    "import json, random, time\n",
    "import google.generativeai as genai\n",
    "\n",
    "def _sanitize_key(k: str) -> str:\n",
    "    if not isinstance(k, str):\n",
    "        return \"\"\n",
    "    # remove all whitespace (spaces, tabs, newlines) and zero-width chars\n",
    "    k = k.replace(\"\\u200b\", \"\").replace(\"\\u200c\", \"\").replace(\"\\u200d\", \"\").replace(\"\\ufeff\", \"\")\n",
    "    k = \"\".join(k.split())\n",
    "    return k\n",
    "\n",
    "# Gemini client init (env var first, fallback to hardcoded string)\n",
    "# If you set GOOGLE_API_KEY in your terminal/.env, you don't need the fallback.\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if google_api_key:\n",
    "    google_api_key = _sanitize_key(google_api_key)\n",
    "else:\n",
    "    # =======================  << PASTE HERE >>  =======================\n",
    "    # Replace the placeholder below with YOUR real key if not using env var.\n",
    "    # Example (FAKE): \"AIzaSyBVWwuubnXhT7...\"\n",
    "    google_api_key = \"\"  # <-- paste your key here\n",
    "    # ==================================================================\n",
    "    google_api_key = _sanitize_key(google_api_key)\n",
    "\n",
    "# Basic validation\n",
    "if not google_api_key or google_api_key.strip() == \"\" or google_api_key.startswith(\"YOUR-\"):\n",
    "    print(\"Error: Google API key not provided.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configure the Gemini client\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "# Create model instance\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "\n",
    "print(\"✅ Google Gemini client ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('runs/20250727_184519')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Paths ===\n",
    "INPUT_JSON = \"prompt_test_2_grouped.json\"  # put this next to the notebook or use absolute path\n",
    "RUNS_ROOT  = Path(\"runs\")\n",
    "\n",
    "# === Models ===\n",
    "GEN_MODEL   = \"gemini-1.5-pro\"   # for generation\n",
    "JUDGE_MODEL = \"gemini-1.5-pro\"   # for judging\n",
    "\n",
    "# === Generation params ===\n",
    "GEN_TEMPERATURE = 0.4\n",
    "GEN_MAX_TOKENS  = 800    # be realistic; too high can error\n",
    "GEN_RETRIES     = 5\n",
    "\n",
    "# === Judge params ===\n",
    "JUDGE_TEMPERATURE = 0.0\n",
    "JUDGE_MAX_TOKENS  = 350\n",
    "JUDGE_RETRIES     = 5\n",
    "\n",
    "# === Fresh run dir ===\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = RUNS_ROOT / ts\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output artifact paths\n",
    "PATH_WITH_ANSWERS = RUN_DIR / \"with_answers.json\"\n",
    "PATH_WITH_SCORES  = RUN_DIR / \"with_scores.json\"\n",
    "PATH_PER_PROMPT   = RUN_DIR / \"per_prompt_scores.csv\"\n",
    "PATH_SUMMARY      = RUN_DIR / \"summary_by_type.csv\"\n",
    "PATH_TRIPLETS     = RUN_DIR / \"triplets_by_topic.csv\"\n",
    "PATH_GEN_LOG      = RUN_DIR / \"gen_log.jsonl\"\n",
    "PATH_JUDGE_LOG    = RUN_DIR / \"judge_log.jsonl\"\n",
    "\n",
    "RUN_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Utilities ready.\n"
     ]
    }
   ],
   "source": [
    "def backoff_wait(attempt: int) -> float:\n",
    "    \"\"\"Exponential backoff with jitter, capped at 60s.\"\"\"\n",
    "    return min(60.0, (2 ** attempt) + random.random())\n",
    "\n",
    "def load_json(path: str) -> Any:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def append_jsonl(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def usage_dict(resp) -> Optional[dict]:\n",
    "    \"\"\"Extract usage information from Gemini response.\"\"\"\n",
    "    try:\n",
    "        # Gemini provides usage metadata differently than OpenAI\n",
    "        if hasattr(resp, 'usage_metadata'):\n",
    "            u = resp.usage_metadata\n",
    "            return {\n",
    "                \"prompt_tokens\": getattr(u, \"prompt_token_count\", None),\n",
    "                \"completion_tokens\": getattr(u, \"candidates_token_count\", None),\n",
    "                \"total_tokens\": getattr(u, \"total_token_count\", None),\n",
    "            }\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"✅ Utilities ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 prompts.\n",
      "Example: {'id': 'Neutral-01', 'type': 'Neutral', 'topic': 'Industrial Revolution', 'question': 'Write a balanced, well‑sourced essay explaining the catalysts, mechanisms, and social—economic consequences of the Industrial Revolution.', 'model_answer': ''}\n"
     ]
    }
   ],
   "source": [
    "data = load_json(INPUT_JSON)\n",
    "items = data.get(\"outputs\") or []\n",
    "print(f\"Loaded {len(items)} prompts.\")\n",
    "print(\"Example:\", items[0] if items else \"N/A\")\n",
    "\n",
    "# Ensure we start clean: drop any lingering model_answer/evaluation in-memory if present\n",
    "for it in items:\n",
    "    it[\"model_answer\"] = \"\"\n",
    "    if \"evaluation\" in it:\n",
    "        del it[\"evaluation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: READY\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Gemini generation config\n",
    "    generation_config = genai.GenerationConfig(\n",
    "        temperature=0,\n",
    "        max_output_tokens=5,\n",
    "    )\n",
    "    \n",
    "    test_model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-pro\",\n",
    "        generation_config=generation_config,\n",
    "        system_instruction=\"You are a concise assistant.\"\n",
    "    )\n",
    "    \n",
    "    test = test_model.generate_content(\"Reply with exactly READY\")\n",
    "    print(\"Sanity check:\", test.text.strip())\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"⚠ Sanity check failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/60] Generating: Neutral-01\n",
      "[2/60] Generating: Supportive-01\n",
      "[3/60] Generating: Threatening-01\n",
      "[4/60] Generating: Neutral-02\n",
      "[5/60] Generating: Supportive-02\n",
      "[6/60] Generating: Threatening-02\n",
      "[7/60] Generating: Neutral-03\n",
      "[8/60] Generating: Supportive-03\n",
      "[9/60] Generating: Threatening-03\n",
      "[10/60] Generating: Neutral-04\n",
      "[11/60] Generating: Supportive-04\n",
      "[12/60] Generating: Threatening-04\n",
      "[13/60] Generating: Neutral-05\n",
      "[14/60] Generating: Supportive-05\n",
      "[15/60] Generating: Threatening-05\n",
      "[16/60] Generating: Neutral-06\n",
      "[17/60] Generating: Supportive-06\n",
      "[18/60] Generating: Threatening-06\n",
      "[19/60] Generating: Neutral-07\n",
      "[20/60] Generating: Supportive-07\n",
      "[21/60] Generating: Threatening-07\n",
      "[22/60] Generating: Neutral-08\n",
      "[23/60] Generating: Supportive-08\n",
      "[24/60] Generating: Threatening-08\n",
      "[25/60] Generating: Neutral-09\n",
      "[26/60] Generating: Supportive-09\n",
      "[27/60] Generating: Threatening-09\n",
      "[28/60] Generating: Neutral-10\n",
      "[29/60] Generating: Supportive-10\n",
      "[30/60] Generating: Threatening-10\n",
      "[31/60] Generating: Neutral-11\n",
      "[32/60] Generating: Supportive-11\n",
      "[33/60] Generating: Threatening-11\n",
      "[34/60] Generating: Neutral-12\n",
      "[35/60] Generating: Supportive-12\n",
      "[36/60] Generating: Threatening-12\n",
      "[37/60] Generating: Neutral-13\n",
      "[38/60] Generating: Supportive-13\n",
      "[39/60] Generating: Threatening-13\n",
      "[40/60] Generating: Neutral-14\n",
      "[41/60] Generating: Supportive-14\n",
      "[42/60] Generating: Threatening-14\n",
      "[43/60] Generating: Neutral-15\n",
      "[44/60] Generating: Supportive-15\n",
      "[45/60] Generating: Threatening-15\n",
      "[46/60] Generating: Neutral-16\n",
      "[47/60] Generating: Supportive-16\n",
      "[48/60] Generating: Threatening-16\n",
      "[49/60] Generating: Neutral-17\n",
      "[50/60] Generating: Supportive-17\n",
      "[51/60] Generating: Threatening-17\n",
      "[52/60] Generating: Neutral-18\n",
      "[53/60] Generating: Supportive-18\n",
      "[54/60] Generating: Threatening-18\n",
      "[55/60] Generating: Neutral-19\n",
      "[56/60] Generating: Supportive-19\n",
      "[57/60] Generating: Threatening-19\n",
      "[58/60] Generating: Neutral-20\n",
      "[59/60] Generating: Supportive-20\n",
      "[60/60] Generating: Threatening-20\n",
      "\n",
      "✅ Generation complete. 60 answers written.\n",
      "Saved → runs/20250727_175338/with_answers.json\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_WRITER = (\n",
    "    \"You are a careful, well-sourced academic writer. \"\n",
    "    \"Answer comprehensively, clearly, and factually. Use concise paragraphs.\"\n",
    ")\n",
    "\n",
    "def generate_one(question: str) -> str:\n",
    "    last_err = None\n",
    "    for attempt in range(GEN_RETRIES):\n",
    "        try:\n",
    "            generation_config = genai.GenerationConfig(\n",
    "                temperature=GEN_TEMPERATURE,\n",
    "                max_output_tokens=GEN_MAX_TOKENS,\n",
    "            )\n",
    "            \n",
    "            writer_model = genai.GenerativeModel(\n",
    "                model_name=GEN_MODEL,\n",
    "                generation_config=generation_config,\n",
    "                system_instruction=SYSTEM_WRITER\n",
    "            )\n",
    "            \n",
    "            resp = writer_model.generate_content(question)\n",
    "            \n",
    "            # Handle potential content filtering\n",
    "            if resp.candidates[0].finish_reason.name in ['SAFETY', 'RECITATION']:\n",
    "                return f\"<FILTERED: {resp.candidates[0].finish_reason.name}>\", resp\n",
    "            \n",
    "            return (resp.text or \"\").strip(), resp\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            wait = backoff_wait(attempt)\n",
    "            print(f\"  ⚠️ gen retry {attempt+1}/{GEN_RETRIES}: {e} (sleep {wait:.1f}s)\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Generation failed after {GEN_RETRIES} retries: {last_err}\")\n",
    "\n",
    "generated = 0\n",
    "for i, it in enumerate(items, 1):\n",
    "    pid = it.get(\"id\", f\"index_{i}\")\n",
    "    q   = it.get(\"question\", \"\")\n",
    "    print(f\"[{i}/{len(items)}] Generating: {pid}\")\n",
    "    try:\n",
    "        answer, resp = generate_one(q)\n",
    "        it[\"model_answer\"] = answer\n",
    "        generated += 1\n",
    "        append_jsonl(PATH_GEN_LOG, {\n",
    "            \"ts\": datetime.utcnow().isoformat()+\"Z\",\n",
    "            \"id\": pid,\n",
    "            \"type\": it.get(\"type\"),\n",
    "            \"topic\": it.get(\"topic\"),\n",
    "            \"usage\": usage_dict(resp),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        it[\"model_answer\"] = f\"<ERROR: {e}>\"\n",
    "        print(f\"   ✗ error: {e}\")\n",
    "\n",
    "# Save answers\n",
    "save_json(PATH_WITH_ANSWERS, {\"outputs\": items})\n",
    "print(f\"\\n✅ Generation complete. {generated} answers written.\")\n",
    "print(f\"Saved → {PATH_WITH_ANSWERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed when ready.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to block until you review the JSON\n",
    "# input(f\"Review answers at: {PATH_WITH_ANSWERS}\\nPress Enter to proceed to judging ... \")\n",
    "print(\"Proceed when ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following with_answers.json files (newest first):\n",
      "\n",
      "[1] runs/20250727_175338/with_answers.json  | items=60  ok=60  empty=0  errors=0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "RUNS_ROOT = Path(\"runs\")  # adjust if your runs folder is elsewhere\n",
    "\n",
    "def count_judgeable(path: Path):\n",
    "    try:\n",
    "        data = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "        items = data.get(\"outputs\", [])\n",
    "        empty = sum(1 for it in items if not (it.get(\"model_answer\") or \"\").strip())\n",
    "        errors = sum(1 for it in items if (it.get(\"model_answer\") or \"\").strip().startswith(\"<ERROR\") \n",
    "                    or (it.get(\"model_answer\") or \"\").strip().startswith(\"<FILTERED\"))\n",
    "        ok = len(items) - empty - errors\n",
    "        return len(items), ok, empty, errors\n",
    "    except Exception as e:\n",
    "        return None, None, None, None\n",
    "\n",
    "cands = sorted(RUNS_ROOT.glob(\"*/with_answers.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not cands:\n",
    "    print(\"No runs/*/with_answers.json found. You may have saved to another folder.\")\n",
    "else:\n",
    "    print(\"Found the following with_answers.json files (newest first):\\n\")\n",
    "    for i, p in enumerate(cands, 1):\n",
    "        total, ok, empty, errors = count_judgeable(p)\n",
    "        print(f\"[{i}] {p}  | items={total}  ok={ok}  empty={empty}  errors={errors}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
